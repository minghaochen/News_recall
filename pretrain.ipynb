{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b871ab2-5211-49df-90c7-6340e6e56e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, LineByLineTextDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7da1c46-8fe4-4c40-a34e-c201734793d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1656: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_255/822080651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save files to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CCF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "\n",
    "paths = './data/vocab.txt'\n",
    "# tokenizer = BertWordPieceTokenizer(paths, lowercase=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(paths)\n",
    "\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(vocab_file=paths, vocab_size=82, min_frequency=2)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\".\", \"CCF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ecfcdc-8b6c-40d0-9359-ddc4761923c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  181813\n",
      "CPU times: user 1min 29s, sys: 736 ms, total: 1min 29s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset= LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "#     file_path = './data/LineText_for_BERT.txt',\n",
    "    file_path = './data/LineText_for_BERT_debug.txt',\n",
    "    block_size = 512  # maximum sequence length\n",
    ")\n",
    "\n",
    "print('No. of lines: ', len(dataset)) # No of lines in your datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60b2e9d-ddb6-4f38-b873-be32a630fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  65199910\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "config = BertConfig(\n",
    "    vocab_size=28198,\n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=6, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    " \n",
    "model = BertForMaskedLM(config)\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719c3c20-0ac5-4792-bbb7-eca3641f27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./pretrain_model',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "#     no_cuda = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1e851-d4ff-41ab-8f2c-fba75337b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 181813\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11364\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model('./pretrain_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec2e1c-61b4-4854-b727-bc9ba5e5baec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
